{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b2b601",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Bash ETL Pipelines, Data Manipulation & Analysis\n",
    "\n",
    "This notebook provides examples and explanations on how to write lightweight ETL pipelines, perform data manipulation, and conduct simple data analysis using **Bash scripting**. It also explores how to embed **SQL queries** within Bash and when to consider using **Python** instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc84328",
   "metadata": {},
   "source": [
    "## üìë Table of Contents\n",
    "1. [What is an ETL Pipeline?](#what-is-an-etl-pipeline)\n",
    "2. [Basic Bash ETL Pipeline Example](#basic-bash-etl-pipeline-example)\n",
    "3. [Common Data Manipulation Techniques](#common-data-manipulation-techniques)\n",
    "4. [Data Analysis in Bash](#data-analysis-in-bash)\n",
    "5. [Embedding SQL in Bash Scripts](#embedding-sql-in-bash-scripts)\n",
    "6. [Python vs Bash for ETL](#python-vs-bash-for-etl)\n",
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820eea4f",
   "metadata": {},
   "source": [
    "## üß© What is an ETL Pipeline?\n",
    "- **Extract**: Pull data from a source (e.g., URL, database, file system).\n",
    "- **Transform**: Clean, filter, or reshape the data.\n",
    "- **Load**: Insert the cleaned data into a database or data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bc835",
   "metadata": {},
   "source": [
    "## üîß Basic Bash ETL Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL=\"https://example.com/data.csv\"\n",
    "RAW=\"data.csv\"\n",
    "CLEAN=\"filtered_data.csv\"\n",
    "DB=\"etl_db\"\n",
    "USER=\"etl_user\"\n",
    "TABLE=\"people\"\n",
    "curl -s -o \"$RAW\" \"$DATA_URL\"\n",
    "awk -F, '$3 > 30' \"$RAW\" > \"$CLEAN\"\n",
    "psql -U $USER -d $DB -c \"\\copy $TABLE(age, name) FROM '$CLEAN' DELIMITER ',' CSV HEADER;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938c471",
   "metadata": {},
   "source": [
    "## üß∞ Common Data Manipulation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "awk -F, 'NF==5' data.csv > cleaned.csv\n",
    "\n",
    "# Filter rows by condition\n",
    "awk -F, '$3 > 30' data.csv > filtered.csv\n",
    "\n",
    "# Convert CSV to JSON\n",
    "csvjson filtered.csv > data.json\n",
    "\n",
    "# Extract column & get top values\n",
    "cut -d',' -f2 data.csv | sort | uniq -c | sort -nr | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37b743",
   "metadata": {},
   "source": [
    "## üìä Data Analysis in Bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787beb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of a column\n",
    "awk -F, '{sum+=$3} END {print \"Average:\", sum/NR}' data.csv\n",
    "\n",
    "# Count number of unique values\n",
    "cut -d',' -f2 data.csv | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888695bf",
   "metadata": {},
   "source": [
    "## üêò Embedding SQL in Bash Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12967efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite example using heredoc\n",
    "sqlite3 etl.db <<'EOF'\n",
    "DELETE FROM people WHERE age IS NULL;\n",
    ".mode csv\n",
    ".import filtered_data.csv people\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U etl_user -d etl_db -c \"DELETE FROM people WHERE age IS NULL;\"\n",
    "psql -U etl_user -d etl_db -c \"\\copy people(age, name) FROM 'filtered_data.csv' CSV HEADER;\"\n",
    "psql -U etl_user -d etl_db -c \"SELECT AVG(age) FROM people;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using external SQL file\n",
    "psql -U etl_user -d etl_db -f stats.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f797953",
   "metadata": {},
   "source": [
    "## üß† Python vs Bash for ETL\n",
    "| Task                            | Bash                     | Python (`pandas`, etc.)         |\n",
    "|----------------------------------|--------------------------|----------------------------------|\n",
    "| File download, parse flat files | ‚úÖ Good                  | ‚úÖ Excellent                     |\n",
    "| Large CSV handling              | ‚ùå Limited by memory     | ‚úÖ With `chunksize`              |\n",
    "| Advanced filtering/grouping     | ‚ùå Complex with `awk`    | ‚úÖ Simple with `pandas`          |\n",
    "| JSON/API support                | ‚ùå `jq` only              | ‚úÖ `requests`, `json`            |\n",
    "| Error handling, logging         | ‚ùå Rudimentary            | ‚úÖ Try/except, `logging`         |\n",
    "| Data visualization              | ‚ùå Not supported          | ‚úÖ `matplotlib`, `seaborn`       |\n",
    "| Testing & modularity            | ‚ùå Minimal                | ‚úÖ `pytest`, `OOP`, packages     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad62509",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion\n",
    "- Bash is excellent for quick scripts, cron jobs, and lightweight ETL\n",
    "- Python is more robust for large-scale, structured, and complex workflows"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
